---
title: "U.S. Loan Default Analysis"
author: "Herve Zumbach, Valentin Frezza" 
date: 2023-10-20
---

## Introduction 

The data for this project comes from: https://www.kaggle.com/datasets/ranadeep/credit-risk-dataset/data. 
The ids used in the data set seems to come from https://lendingclub.com, a U.S. financial services company specialized in peer-to-peer lending. 
The authenticity of the data could not be established; the size of the data set would indicate that maybe the data was automatically generated, however the reference to actual lendingclub ids seems to contradict this. 
The website for lendingclub does not offer data about its clients. 

The main research question tackled in this data analysis was which factor affected the probability of default the most. 
To answer it, automatic visualization of the data was used in large scale so as to understand quickly and efficiently which variable had the most effect on the default probability. 

## Data Loading and Tidying 

To begin our analysis, we started by downloading the necessary libraries. In addition we also generated some vectors for clean graphical representation of the data. 
```{r}
# Load the libraries for our analysis
library(dplyr)
library(stringr)
library(ggplot2)
library(gridExtra)
library(knitr)
library(usmap)
library(glmnet)
library(viridis)
library(tidyr)

# Create some vector for automatic labelization in our graphs 
clean_columns_names <- c(
  "term" = "Term", 
  "loan_amnt" = "Loan amount", 
  "int_rate" = "Interest rate", 
  "grade" = "Grade", 
  "sub_grade" = "Sub-grade", 
  "annual_inc" = "Annual Income", 
  "loan_status" = "Loan status", 
  "purpose" = "Purpose", 
  "us_state" = "U.S. state", 
  "open_acc" = "Open account", 
  "total_acc" = "Total account", 
  "total_pymnt" = "Total payment", 
  "years_employment" = "Years of employment", 
  "out_prncp" = "Out principal", 
  "loan_per_income" = "Loan per income", 
  "funded_amnt" = "Funded amount", 
  "politics" = "Political party"
)

ordered_factor = c(
  "term" = FALSE,
  "grade" = TRUE,
  "sub_grade" = TRUE,
  "purpose" = FALSE, 
  "years_employment" = TRUE, 
  "us_state" = FALSE, 
  "politics" = FALSE
)
```

## First impression

Looking at the data, the first issue was the size of the data set in terms  of number of observations as well as variables. 
With 74 columns and 887'379 rows, our data set was simply to large for fast computation and made choosing the correct variables for our analysis challenging. 

```{r}
# Load the dataset
data_loan <- read.csv("../data/loan.csv")

# Get a first impression 
kable(summary(data_loan))
```

## First filter 

To simplify computation and make the data set manageable, we only look at the fully paid or defaulted loans making the data set go from 878'379 entries to 208'942. 
In addition, we also reduce the columns to 15 to only keep pertinent entries. 

```{r}
# Focusing only on fully paid and default
data_loan <- filter(data_loan,
                    loan_status == "Default" | loan_status == "Fully Paid" )

# Select only the important variables
data_loan <- select(data_loan,id,loan_amnt,funded_amnt,term,int_rate,grade,sub_grade,
                    annual_inc,loan_status,purpose,addr_state,open_acc,total_acc,
                    total_pymnt,emp_length)

# Get a second impression
kable(summary(data_loan))
```

## Second Filter 

We also remove all the entries with no value for the number of years of employment: 

```{r}
# Counting number of cells without any value for the variable year of employment
data_loan |>
  select(id,emp_length)|>
  group_by(emp_length)|>
  summarize( n_distinct(id))|> 
  ungroup()

# There are 7443 n/a that we want to get rid of
data_loan <- filter(data_loan, data_loan$emp_length != "n/a")
```

## Adding new entries 

We add the columns politics and loan_per_income. 
The columns politics represent wether the state of the debtor is republican or democrat whereas the loan_per_income column represent the amount of loan divided by the yearly income of the debtor. 

```{r}
# Creation of the loan_per_income column
data_loan <- mutate(data_loan,
                    loan_per_income = loan_amnt / annual_inc)

# Adding a field to know whether the state votes democrat or republican
rep <-c("TX","OK","AR","LA","MS","AL","FL","TN","SC","KY","NC","WV","MT","ID",
        "WY","UT","AK","ND","SD","IA","NE","KS","MO","IN","OH")
dem <-c("VA","GA","DE","MD","HI","AZ","NM","CO","NV","CA","OR","WA","MN","ME",
        "NH","MA","VT","RI","CT","NY","PA","NJ","DC","IL","MI","WI")

data_loan$politics[data_loan$addr_state %in% rep] <- "REP"
data_loan$politics[data_loan$addr_state %in% dem] <- "DEM"
```

## Cosmetic Changes 

We change a few variables names for easier handling, renamed the ids and made the interests rate actual percentage: 

```{r}
# rename variables
data_loan <-data_loan %>%
  rename("us_state"="addr_state",
         "years_employment"="emp_length")

# changing ids
data_loan["id"]<-c(str_c("id_",c(1:nrow(data_loan))))

#We want the interest rates values to be percentages
data_loan$int_rate = data_loan$int_rate / 100
```

## Factorisation 

We factorize all the categorical data columns: 

```{r}
# get a list of categorical data 
factor_names <- c("loan_status", "term", "grade", "sub_grade", "purpose", 
                  "years_employment", "us_state", "politics")

# check that the factor are coherent 
for (factor_name in factor_names){
  unique_values <- unique(data_loan[, factor_name])
  print(unique_values)
}

# factorize them 
for (factor_name in factor_names){
  data_loan[, factor_name] <- factor(data_loan[, factor_name])
}
```

## Extreme Values Handling    

A few of the values in our data set are too extreme for proper analysis, so we decided to filter them. 

## Extreme Values visualization

To do so, we filter the 99th quantile of the data: 

```{r}
# Define the vector of continuous values 
cont_data_names <- c("loan_amnt", "int_rate", "annual_inc", "total_pymnt", 
                     "loan_per_income", "open_acc", "total_acc")

# Copy the data to see how it changes with the filter 
data_old <- data_loan

# filter the 99% percentile 
for (cont_data_name in cont_data_names){
  threshold <- quantile(data_loan[[cont_data_name]], probs = 0.99)
  print(paste0("The data in ", clean_columns_names[cont_data_name], " is filtered with threshold: ", threshold))
  extreme_data <- filter(data_loan, !!sym(cont_data_name) >= threshold)
  data_loan <- filter(data_loan, !!sym(cont_data_name) < threshold)
}
```

## Extreme Values Filtering 

We then check how the filter affected our data with some boxplots: 

```{r}
# Get a representation of the extremes values filter with some boxplots 
for (cont_data_name in cont_data_names){
  
  plt_2 <- ggplot(data_old, aes_string(x = 1, y = cont_data_name )) + 
    geom_boxplot(outlier.colour = "red",  outlier.shape = 1) +
    theme_minimal() + 
    labs(x = clean_columns_names[cont_data_name], 
         y = "", 
         title = "Unfiltered Data")
  
  plt_1 <- ggplot(data_loan, aes_string(x = 1, y = cont_data_name )) + 
    geom_boxplot(outlier.colour = "red",  outlier.shape = 1) +
    theme_minimal() + 
    labs(x = clean_columns_names[cont_data_name], 
         y = "", 
         title = "Filtered Data")
  
  grid.arrange(plt_1, plt_2, ncol = 2)
}
```

## Data Analysis

In this section, we focus on getting a good understanding of our data through visual representation. 
Due to the large amount of columns in our data set, we choose an approach based on looping to get through the data. 

## Graphical Analysis Functions 

To do so, we begin by defining some functions to plot both our continuous and categorical data. 

```{r}
# Plot functions ---------------------------------------------------------------

# Utils ........................................................................

# Find the angle to write the text for the graph 
find_angle <- function(data, factor_name){
  n_level <- nlevels(data[, factor_name])
  if (n_level < 8){
    angle = 0
  } else if ( n_level < 15){
    angle = 45
  } else {
    angle = 90 
  }
  
  return(angle)
}

# Categorical variables functions ..............................................

# Plot the default probability for each categorical variable 
plot_cat_relative_default <- function(data, factor_name) {

  # Get a summary dataframe 
  summary <- data |>
    group_by(across({{ factor_name }}))|>
    summarize(default_probability = mean(loan_status=="Default"), 
              n = n()) |>
    ungroup()
  
  if (!ordered_factor[factor_name]){
    summary <- summary[order(summary$default_probability),]
    factors <- as.character(summary[[factor_name]])
    summary[[factor_name]] <- factor(factors, levels=factors) 
  }

  plt <- ggplot(summary,  aes_string(x = "default_probability", y = 
                                         factor_name, fill = "default_probability"))+
    geom_bar(stat = "identity")+ 
    labs(x = "Percentage of default", 
         y = clean_columns_names[factor_name], 
         fill = "Probability of default") + 
    scale_fill_viridis(direction = -1) + 
    scale_y_discrete(labels = function(x) gsub("_", " ", x)) + 
    theme_minimal()  

  print(plt)
}

# Plot the categorical variables with the default and the paid debt 
plot_cat_default <- function(data, factor_name) {
  angle <- find_angle(data, factor_name)
  
  plt <- ggplot(data, aes_string(x = factor_name, fill = "loan_status"))+
    geom_bar() + 
    labs(x = clean_columns_names[factor_name], 
         y = "Count", 
         fill = "Loan status") + 
    theme(legend.position = "bottom") +
    scale_fill_viridis(discrete=TRUE, direction = -1) + 
    theme_minimal() + 
    theme(axis.text.x = element_text(angle = angle, hjust = 1)) + 
    scale_x_discrete(labels = function(x) gsub("_", " ", x))
    
  print(plt)
}

# Continuous variables functions ...............................................

# Plot the continuous variable histogram of what is paid and what is defaulted 
plot_cont_default_n_paid <- function(data, cont_name) {
  plt <- ggplot(data, aes_string(x = cont_name, fill = "loan_status"))+
    geom_histogram() + 
    labs(x = clean_columns_names[cont_name], 
         y = "Count", 
         fill = "Count") + 
    scale_fill_viridis(discrete = TRUE, direction = -1) + 
    theme_minimal()
  
  print(plt)
}

# Plot the defaults for continuous value in absolute terms  
plot_cont_default <- function(data, cont_name) {
  
  default_data <- filter(data, loan_status == "Default")
  plt <- ggplot(default_data, aes(x = !!sym(cont_name), fill=..count..))+
    geom_histogram() + 
    labs(x = clean_columns_names[cont_name], 
         y = "Count", 
         fill = "Count") + 
    scale_fill_viridis(direction = -1) +
    theme_minimal()
  
  print(plt)
}

# Plot the percentage of default for each continuous data 
plot_cont_default_percentage <- function(data, column) {

  # break down the data in 30 bins, find the percentage of default for each bin
  percentage <- data |>
    group_by(cut(!!sym(column), breaks = 30)) |>
    summarize(default_percentage = mean(loan_status == "Default")) |>
    ungroup()
  
  # rename the column for plotting 
  colnames(percentage) <- c("bins", "default_percentage")
  
  # create a vector for the x-axis 
  min_x <- min(data[[column]])
  max_x <- max(data[[column]])
  x_axis <- seq(from = min_x, to = max_x, length.out = nrow(percentage))
  
  # add the x axis to the percentage 
  percentage$x_axis <- x_axis 
  
  # plot the percentage 
  plt <- ggplot(percentage, aes(x = x_axis, y = default_percentage, fill= default_percentage)) +
    geom_bar(stat = "identity") + 
    labs(x = clean_columns_names[column], 
         y = "Percentage of default", 
         fill = "Default percentage") + 
    scale_fill_viridis(direction = -1) +
    theme_minimal()
  
  print(plt)
}
```

## Categorical Data Plotting 

We then loop through all the categorical data to understand how they affect the probability of default looking first at the absolute number of observation and then at the relative default probability of each categorical data. 

```{r}
# Categorical Data plotting ....................................................

cat_data_names <- c("term", "grade", "sub_grade", "purpose", 
                    "years_employment", "us_state")

# General view 
for (cat_data in cat_data_names){
  plot_cat_default(data_loan, cat_data)
}

# Probability of default 
for (cat_data in cat_data_names){
  plot_cat_relative_default(data_loan, cat_data)
}
```

## Continuous Data Plotting 

We also loop through all the continuous data to see how they affect the probability of default. 
To do so, the continuous data were put into 30 bins of equal length. 
The number of default and of debt repaid was then plotted as well as the relative default probability for each bin. 

```{r}
# Continuous Data plotting .....................................................

# General view 
for (cont_data in cont_data_names){
  plot_cont_default_n_paid(data_loan, cont_data)
}

# Number of defaults 
for (cont_data in cont_data_names){
  plot_cont_default(data_loan, cont_data)
}

# Default percentage 
for (cont_data in cont_data_names) {
  plot_cont_default_percentage(data_loan, cont_data)
}
```

## Political Analysis 

In this part, we want to understand the differences between Republicans and Democrats.

## Political Analysis Plotting Functions 

To do so, we use the same approach we used before, that is defining some plotting functions for our categorical and continuous data, and then looping through them with all of our data:  

```{r}
# Functions --------------------------------------------------------------------

# Categorical Data .............................................................

# Plot the absolute distribution for republicans and democrats across all the 
# categorical data 
plot_cat_pol_default <- function(data, factor_name) {
  # find the angle for the x-axis scale
  angle <- find_angle(data, factor_name)
  
  # create the graph 
  plt <- ggplot(data, aes_string(x = factor_name, fill = "politics"))+
    geom_bar(position = "dodge")+ 
    labs(x = clean_columns_names[factor_name], 
         y = "Count", 
         fill = clean_columns_names["politics"]) + 
    theme(legend.position = "bottom") +
    scale_fill_manual(values = c("REP" = "red", "DEM" = "blue")) + 
    theme_minimal() + 
    theme(axis.text.x = element_text(angle = angle, hjust = 1)) + 
    scale_x_discrete(labels = function(x) gsub("_", " ", x))
  
  print(plt)
}

# Plot the probability of default for republican or democrats for each 
# categorical data
plot_cat_pol_relative_default <- function(data, factor_name) {
  summary <- data |>
    group_by(across({{ factor_name }}))|>
    summarize(DEM = mean(loan_status=="Default" & politics=="REP"), 
              REP = mean(loan_status=="Default" & politics=="DEM")) |>
    ungroup()
  
  summary <- pivot_longer(summary, c("REP", "DEM"),
                          names_to = "politics", values_to = "default_probability")
  
  # find the angle for the x-axis scale
  angle <- find_angle(data, factor_name)
  
  plt <- ggplot(summary, aes(x = !!sym(factor_name), y =default_probability, fill=politics))+ 
    geom_bar(stat = "identity", position = "dodge") + 
    labs(x = clean_columns_names[factor_name], 
         y = "Probability of default", 
         fill = clean_columns_names["politics"]) + 
    theme(legend.position = "bottom") +
    scale_fill_manual(values = c("REP" = "red", "DEM" = "blue")) + 
    theme_minimal() + 
    theme(axis.text.x = element_text(angle = angle, hjust = 1)) + 
    scale_x_discrete(labels = function(x) gsub("_", " ", x))
  
  print(plt)
}


# Plot The density of the continuous data distribution for both republicans and
# democrats 
plot_density_pol <- function(data, factor_name){
  
  # Find the mean for the republicans and the democrats
  mean_dem <- mean(data[, factor_name][data$politics == "DEM"])
  mean_rep <- mean(data[, factor_name][data$politics == "REP"])
  
  # Set the x position for the mean label 
  x_range <- max(data[,factor_name]) - min(data[, factor_name])
  if (mean_dem < mean_rep){
    dem_x_pos <- -(0.1 * x_range)
    rep_x_pos <- (0.1 * x_range)
  } else {
    dem_x_pos <- (0.1 * x_range)
    rep_x_pos <- -(0.1 * x_range)
  }
  
  # Create the plot 
  plt <- ggplot(data = data, aes(x = !!sym(factor_name),y=after_stat(scaled), color = politics)) +
    geom_density(alpha = 0.5, linewidth = 1.5)+
    geom_vline(xintercept = c(mean_dem, mean_rep),
               color = c("blue", "red"), 
               linetype = "dashed", size = 1) +
    annotate("text", x = mean_dem + dem_x_pos, y = 0.1, 
             label = round(mean_dem, 2), 
             color = "blue", size = 4, vjust = 0) +
    annotate("text", x = mean_rep + rep_x_pos, y = 0.1,
              label = round(mean_rep, 2), 
              color = "red", size = 4, vjust = 0) +
    labs(x = clean_columns_names[factor_name],
         y="Density", 
         color =  clean_columns_names["politics"]) +
    theme_minimal() + 
    scale_color_manual(values = c("blue", "red"))

  print(plt)
}

# Continuous Data ..............................................................

# Plot the continuous variable histogram of what is paid and what is defaulted 
plot_cont_pol <- function(data, column) {
  plt <- ggplot(data, aes_string(x = column, fill = "politics"))+
    geom_histogram(position = "dodge") + 
    theme_minimal() + 
    scale_fill_manual(values = c("REP" = "red", "DEM" = "blue")) + 
    labs(x = clean_columns_names[column], 
         y = "Count", 
         fill = clean_columns_names["politics"])
    
  print(plt)
}

```

## Plotting for Categorical Data 

We then plot all the categorical data along the division line of politics: 

```{r}
# Categorical Data .............................................................

plot_cat_relative_default(data_loan, "politics")

pol_cat_data_names <- c("term", "grade", "sub_grade", "purpose", "years_employment")

for (cat_data in pol_cat_data_names){
  plot_cat_pol_default(data_loan, cat_data)
} 
  
for (cat_data in pol_cat_data_names){
  plot_cat_pol_relative_default(data_loan, cat_data)
}

```

## Plotting for Continuous Data 

And the same for continuous data: 

```{r}
# Continuous Data ..............................................................

for (cont_data in cont_data_names){
  plot_cont_pol(data_loan, cont_data)
}

for (cont_data in cont_data_names){
  plot_density_pol(data_loan, cont_data)
}
```

## Modelling 

In this part, we foccus on modelling our data with the use of the glmnet library. 

## Checking For the Correlation 

First, we can check the coeficients of correlation of our continuous data: 

```{r}
# Checking the correlation -- --------------------------------------------------

cont_data_names <- c("loan_amnt", "int_rate", "annual_inc", "total_pymnt", 
                     "loan_per_income", "open_acc", "total_acc")

# check for redundant data with correlation
correlation_matrix <- cor(data_loan[cont_data_names])

# Create a heatmap to visualize the resulting correlation matrix. 
ggplot(data = data.frame(x = rep(colnames(correlation_matrix), each = 
                                   ncol(correlation_matrix)),
                         y = rep(colnames(correlation_matrix), times = 
                                   ncol(correlation_matrix)),
                         corr = as.vector(correlation_matrix)), 
       aes(x = x, y = y, fill = corr)) +
  geom_tile() +
  scale_fill_viridis(direction = 1, limits = c(-1, 1)) + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ 
  labs(fill = "Correlation") + 
  scale_x_discrete(labels = function(x) clean_columns_names[x]) + 
  scale_y_discrete(labels = function(x) clean_columns_names[x]) + 
  theme(axis.title.x = element_blank(), axis.title.y = element_blank())
```

## GLMNET 

Then we use glmnet to model our data: 

```{r}
# Modelling --------------------------------------------------------------------

# Prepare the data   
x_data <- data_loan %>%
  select(- loan_status, - id)
y_data <- as.numeric(data_loan$loan_status)

# Compute the model using GLMnet 
model <- glmnet(x=x_data, y=y_data, alpha = 1)

plot(model)

```
## Analysis and Conclusion

As was shown in our results, most of the categorical and continuous data showed an influence on the default probability. 
It is however important to note the influence of the number of data point for the certainty of the probability obtained from them. 
For the categorical data, the clearest influence was that of the grade of the loan, for the continuous data, we can note the influence of the loan per income variable or that of the interest rate. 

Interestingly, the politic of the state from which each debtor came from seems to have an influence on the default probability as well. 

For the modeling part, many questions remain unanswered and the GLM analysis proved very tricky to use and its accuracy hard to measure. 

Further work could focus on including the variance of each probability estimate as well as an improved modeling approach. 
